{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading data...\n",
      "train images loaded:  2000\n",
      "train labels loaded:  2000\n",
      "test images loaded:  700\n",
      "solution images loaded:  700\n",
      "test, solution ids loaded:  700 , 700\n",
      "y_train loaded:  2000 \n",
      "\n",
      "\n",
      "initializing tensors...\n",
      "train tensors created\n",
      "test tensors created\n",
      "solution tensors created\n",
      "train tensor of tensor created: (2000, 100, 100)\n",
      "test tensors of tensors created: (700, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# import np_utils # pip install np-utils\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, Conv2D\n",
    "\n",
    "\n",
    "def load_data(Train_df,idx,\n",
    "              batch_size):\n",
    "    df = pd.read_csv(\n",
    "                  Train_df, skiprows=idx*batch_size,\n",
    "                  nrows=batch_size)\n",
    "    x = df.iloc[:,1:]\n",
    "         \n",
    "    y = df.iloc[:,0]\n",
    "    return (np.array(x), np_utils.to_categorical(y))\n",
    "\n",
    "\n",
    "def batch_generator(Train_df,batch_size,\n",
    "                    steps):\n",
    "    idx=1\n",
    "    while True: \n",
    "        yield load_data(Train_df,idx-1,batch_size)## Yields data\n",
    "        if idx<steps:\n",
    "            idx+=1\n",
    "        else:\n",
    "            idx=1\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# DATA UPLOAD\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Total number of train and test points\n",
    "MAX_TRAIN = 178158\n",
    "MAX_TEST = 22596\n",
    "\n",
    "SUBSET_TRAIN = 1000\n",
    "SUBSET_TEST = 700\n",
    "\n",
    "num_train = SUBSET_TRAIN\n",
    "num_test = SUBSET_TEST\n",
    "\n",
    "width = 100\n",
    "height = 100           \n",
    "     \n",
    "label_names = [\"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\n",
    "               \"Lung Opacity\",\"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\n",
    "               \"Atelectasis\",\"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\n",
    "               \"Fracture\",\"Support Devices\"]\n",
    "\n",
    "id_header = 'Id,No Finding,Enlarged Cardiomediastinum,Cardiomegaly,Lung Opacity,Lung Lesion,Edema,Consolidation,Pneumonia,Atelectasis,Pneumothorax,Pleural Effusion,Pleural Other,Fracture,Support Devices'\n",
    "\n",
    "# For full data use:\n",
    "# ROOT_PATH = '/central/groups/CS156b/teams/clnsh/compressed_data/'\n",
    "\n",
    "# For subset data use: \n",
    "ROOT_PATH = '/central/groups/CS156b/teams/clnsh/subset_compressed_data/subset_'\n",
    "\n",
    "print('uploading data...')\n",
    "train_loaded = np.loadtxt(ROOT_PATH + 'train_matrices')\n",
    "num_train = int(train_loaded.size / (width * height))\n",
    "train = train_loaded.reshape(num_train, (width*height) // width, width)\n",
    "\n",
    "print('train images loaded: ', len(train))\n",
    "\n",
    "train_label = pd.read_table(ROOT_PATH + 'train_labels', delimiter=\" \", \n",
    "              names=label_names)\n",
    "\n",
    "print('train labels loaded: ', len(train_label))\n",
    "\n",
    "test_loaded = np.loadtxt(ROOT_PATH + 'test_matrices')\n",
    "num_test = int(test_loaded.size / (width * height))\n",
    "test = test_loaded.reshape(num_test, (width*height) // width, width)\n",
    "\n",
    "print('test images loaded: ', len(test))\n",
    "\n",
    "solution_loaded = np.loadtxt(ROOT_PATH + 'solution_matrices')\n",
    "num_solution = int(solution_loaded.size / (width * height))\n",
    "solution = solution_loaded.reshape(num_solution, (width*height) // width, width)\n",
    "\n",
    "print('solution images loaded: ', len(solution))\n",
    "\n",
    "test_img_id = np.loadtxt(ROOT_PATH + 'test_img_ids')\n",
    "solution_img_id = np.loadtxt(ROOT_PATH + 'solution_img_ids')\n",
    "\n",
    "print('test, solution ids loaded: ', len(test_img_id), ',', len(solution_img_id))\n",
    "\n",
    "# forming labels of training data\n",
    "y_train = []\n",
    "for index, row in train_label.iterrows():\n",
    "    temp2 = [row['No Finding'], row['Enlarged Cardiomediastinum'], row['Cardiomegaly'],\n",
    "            row['Lung Opacity'], row['Lung Lesion'], row['Edema'], row['Consolidation'],\n",
    "            row['Pneumonia'], row['Atelectasis'], row['Pneumothorax'], row['Pleural Effusion'], \n",
    "            row['Pleural Other'], row['Fracture'], row['Support Devices']]\n",
    "    i = 0 \n",
    "    for val in temp2: \n",
    "        if val != val: # Handles NaN's\n",
    "            temp2[i] = 0.0\n",
    "        i += 1\n",
    "    y_train.append(temp2)\n",
    "\n",
    "print('y_train loaded: ', len(y_train), '\\n\\n')\n",
    "X_train = train\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Forming lists of tensored images\n",
    "print('initializing tensors...')\n",
    "tensor_train = []\n",
    "tensor_test = []\n",
    "tensor_solution = []\n",
    "\n",
    "for i in range(0, len(train)):\n",
    "    tensor_train.append(tf.convert_to_tensor(train[i], dtype=float))\n",
    "print('train tensors created')\n",
    "\n",
    "for i in range(0, len(test)):\n",
    "    tensor_test.append(tf.convert_to_tensor(test[i], dtype=float))\n",
    "print('test tensors created')\n",
    "\n",
    "for i in range(0, len(solution)):\n",
    "    tensor_solution.append(tf.convert_to_tensor(solution[i], dtype=float))\n",
    "print('solution tensors created')\n",
    "\n",
    "# setting training and testing data to be a tensor of tensor-ed images\n",
    "# shape: n training images, shape width, shape height\n",
    "train_tensor_of_tensors = tf.convert_to_tensor(tensor_train)\n",
    "print('train tensor of tensor created:', train_tensor_of_tensors.shape)\n",
    "\n",
    "test_tensor_of_tensors = tf.convert_to_tensor(tensor_test)\n",
    "print('test tensors of tensors created:', test_tensor_of_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 15s 717ms/step - loss: 6.8067 - mean_squared_error: 6.8067\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 14s 721ms/step - loss: 0.2821 - mean_squared_error: 0.2821\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 14s 719ms/step - loss: 0.2821 - mean_squared_error: 0.2821\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 14s 723ms/step - loss: 0.2821 - mean_squared_error: 0.2821\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 14s 712ms/step - loss: 0.2821 - mean_squared_error: 0.2821\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 14s 711ms/step - loss: 0.2821 - mean_squared_error: 0.2821\n",
      "Epoch 7/10\n",
      " 4/20 [=====>........................] - ETA: 11s - loss: 0.2837 - mean_squared_error: 0.2837"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# MODEL\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "num_classes = 14\n",
    "num_batch = 100\n",
    "num_epoch = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', \n",
    "                 input_shape=[width, height, 1]))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='relu'))\n",
    "\n",
    "model.compile(loss= tf.keras.losses.MeanSquaredError(),# keras.losses.categorical_crossentropy, mse loss\n",
    "              optimizer= keras.optimizers.Adam(learning_rate=0.01), # adam optimizer\n",
    "              metrics=['mean_squared_error']) # MSE accuracy\n",
    "\n",
    "model.fit(train_tensor_of_tensors, y_train, epochs=num_epoch, verbose=1, batch_size=num_batch)\n",
    "\n",
    "print('model fit complete')\n",
    "\n",
    "\n",
    "preds = model.predict(test_tensor_of_tensors)\n",
    "preds = preds.tolist()\n",
    "\n",
    "print('predictions complete')\n",
    "\n",
    "scaled_preds = []\n",
    "for i in range(len(preds)):\n",
    "    curr = np.array(preds[i])\n",
    "    factor = min(curr)\n",
    "    newList = [(x - factor) for x in curr]\n",
    "    max_curr = max(newList)\n",
    "    myInt = max_curr / 2\n",
    "    newList2 = [((x / myInt)-1) for x in newList]\n",
    "    scaled_preds.append(newList2)\n",
    "\n",
    "print('scaled predictions complete')\n",
    "\n",
    "\n",
    "for i in range(len(tensor_test)):\n",
    "    img_id = test_img_id[i]\n",
    "    scaled_preds[i].insert(0, img_id)\n",
    "\n",
    "np.set_printoptions(suppress=True) # Stop ID's from being converted to scientific notation\n",
    "np.savetxt(\"test_sub.csv\", scaled_preds, delimiter=\",\", fmt='%f', header = id_header, comments = '')\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
