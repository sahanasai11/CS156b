{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3ddb1f-548a-403c-8dd6-5004ec0a70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt #!pip install keras-tuner -q\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e940e7ef-3a51-4b15-aaf2-173e1432f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "Model builds with hyperparameters, which is defining the search space\n",
    "'''\n",
    "\n",
    "\n",
    "def model_build_curr(hyper_param):\n",
    "    # our current model \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(100,100,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(3,3)))\n",
    "    model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1024,activation='relu'))\n",
    "    model.add(keras.layers.Dense(1024,activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(1024,activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(14,activation='sigmoid'))\n",
    "\n",
    "\n",
    "    # tuning the units \n",
    "    # an integer hyperparameter with an inclusive range from 32 to 512 \n",
    "    # Also, going through the inteveral has a minimum step of 32 \n",
    "    first_hp = hyper_param.Int('units', min_value=32, max_value=512, step=32)\n",
    "\n",
    "    #activation parameter\n",
    "    # default is relu \n",
    "    default_activation = 'relu'\n",
    " \n",
    "    #for simplicity, we are going to add two Dense layers while tuning the number of units \n",
    "    # in the first layer \n",
    "    model.add(layers.Dense(units=first_hp, activation='relu'))\n",
    "    # change the parameters of the model to correspond with corresponding model \n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    print(hyper_param.Int(\"units\", min_value=32, max_value=512, step=32))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be325595-e934-4232-84a6-c051f4ee2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_build_boolean(hyper_param):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # tuning the units \n",
    "    # an integer hyperparameter with an inclusive range from 32 to 512 \n",
    "    # Also, going through the inteveral has a minimum step of 32 \n",
    "    first_hp = hyper_param.Int('units', min_value=32, max_value=512, step=32)\n",
    "\n",
    "    #activation parameter\n",
    "    # tuning which activation function to use\n",
    "    activation_level = hyper_param.Choice('activation', ['relu', 'tanh'])\n",
    "   \n",
    "    model.add(layers.Dense(units=first_hp, activation=activation_level))\n",
    "\n",
    "    # use boolean proccess to tune with ot without a Dropout Layer \n",
    "    drate = .25\n",
    "    if hyper_param.Boolean('dropout'):\n",
    "        model.add(layers.Dropout(rate=drate))\n",
    "    \n",
    "    # continue to add softmax layer \n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # tun the optimizer learning rate (i.e. define as a hyperparameter)\n",
    "    lrate = hyper_param.Float('lr', min_value = .00001, max_value= .01, sampling='log')\n",
    "    optimizer_lr = keras.optimizers.Adam(learning_rate=lrate)\n",
    "    model.compile(optimizer=optimizer_lr, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(hyper_param.Int(\"units\", min_value=32, max_value=512, step=32))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e99a6b1e-3e93-4102-82a4-a9950f38570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "Starting the search and importing the data. \n",
    "Possible tuner classes: RandomSearch, Hyperband, BayesianOptimization\n",
    "'''\n",
    "def set_up_tuner(objective, maxt, executions, over_write, direct, proj_name):\n",
    "# initialization of tuner, need to fill in directory and project_name depedning on who is running program\n",
    "    tuner = kt.RandomSearch(\n",
    "        hypermodel = model_build_curr, \n",
    "        objective = objective, #'val_accuracy'\n",
    "        max_trials = maxt, \n",
    "        executions_per_trial=executions, \n",
    "        overwrite=over_write, #boolean\n",
    "        directory=direct, \n",
    "        project_name=proj_name\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    return tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27ec452-946f-4892-8d4f-5f9a1e5d0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_tuner(tuner, x_train, y_train, eps, x_validation, y_validation, no_models, x, y, z):\n",
    "    # searches for the best hyperparameter configuration \n",
    "    # note, all values passed into search are also passed into model.fit()\n",
    "    tuner.search(x_train, y_train, epochs=eps, validation_data=(x_validation, y_validation))\n",
    "    tuner.results_summary() \n",
    "\n",
    "    # get results \n",
    "    final_models = tuner.get_best_models(num_models=no_models)\n",
    "    best = final_models[0]\n",
    "\n",
    "    # build the best model \n",
    "    best.build(input_shape=(x, y, z))\n",
    "    best.summary()\n",
    "\n",
    "    # logs are located in directory/project_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e11e931-84c9-4cf7-9508-d5fb4380f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Rebuilding the model with the best hyperparameters\n",
    "'''\n",
    "def retrain_model(tuner, x_train, y_train, eps, x_validation, y_validation):\n",
    "    # get the top 2 params\n",
    "    best_params = tuner.get_best_hyperparameters(5)\n",
    "    model = model_build_boolean(best_params[0])\n",
    "\n",
    "    # have to fit the entire dataset \n",
    "    x_new = np.concatenate((x_train, x_validation))\n",
    "    y_new = np.concatenate((y_train, y_validation))\n",
    "\n",
    "    # epochs should be 1 mostly\n",
    "    model.fit(x=x_new, y=y_new, epochs=eps)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d327a0eb-bab7-4a15-93d9-132bf2175520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading data...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/central/groups/CS156b/teams/clnsh/subset_compressed_data10/subset_train_matrices not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m ROOT_PATH2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/central/groups/CS156b/teams/clnsh/compressed_data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muploading data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m train_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_matrices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m num_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_loaded\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m/\u001b[39m (width \u001b[38;5;241m*\u001b[39m height))\n\u001b[1;32m     30\u001b[0m train \u001b[38;5;241m=\u001b[39m train_loaded\u001b[38;5;241m.\u001b[39mreshape(num_train, (width\u001b[38;5;241m*\u001b[39mheight) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m width, width)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/lib/npyio.py:1065\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_string_like(fname):\n\u001b[0;32m-> 1065\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m     fencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1067\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(fh)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/lib/_datasource.py:194\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/lib/_datasource.py:531\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    529\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m path)\n",
      "\u001b[0;31mOSError\u001b[0m: /central/groups/CS156b/teams/clnsh/subset_compressed_data10/subset_train_matrices not found."
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_TRAIN = 178158\n",
    "MAX_TEST = 22596\n",
    "\n",
    "SUBSET_TRAIN = 2000\n",
    "SUBSET_TEST = 700\n",
    "\n",
    "num_train = MAX_TRAIN\n",
    "num_test = MAX_TEST\n",
    "\n",
    "width = 100\n",
    "height = 100           \n",
    "     \n",
    "label_names = [\"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\n",
    "               \"Lung Opacity\",\"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\n",
    "               \"Atelectasis\",\"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\n",
    "               \"Fracture\",\"Support Devices\"]\n",
    "\n",
    "id_header = 'Id,No Finding,Enlarged Cardiomediastinum,Cardiomegaly,Lung Opacity,Lung Lesion,Edema,Consolidation,Pneumonia,Atelectasis,Pneumothorax,Pleural Effusion,Pleural Other,Fracture,Support Devices'\n",
    "\n",
    "# For full data use:\n",
    "#ROOT_PATH = '/central/groups/CS156b/teams/clnsh/compressed_data/'\n",
    "\n",
    "# For subset data use: \n",
    "ROOT_PATH = '/central/groups/CS156b/teams/clnsh/subset_compressed_data10/subset_'\n",
    "ROOT_PATH2 = '/central/groups/CS156b/teams/clnsh/compressed_data/'\n",
    "\n",
    "print('uploading data...')\n",
    "train_loaded = np.loadtxt(ROOT_PATH + 'train_matrices')\n",
    "num_train = int(train_loaded.size / (width * height))\n",
    "train = train_loaded.reshape(num_train, (width*height) // width, width)\n",
    "\n",
    "print('train images loaded: ', len(train))\n",
    "\n",
    "train_label = pd.read_table(ROOT_PATH + 'train_labels', delimiter=\" \", \n",
    "              names=label_names)\n",
    "\n",
    "print('train labels loaded: ', len(train_label))\n",
    "\n",
    "test_loaded = np.loadtxt(ROOT_PATH2 + 'test_matrices')\n",
    "num_test = int(test_loaded.size / (width * height))\n",
    "test = test_loaded.reshape(num_test, (width*height) // width, width)\n",
    "\n",
    "print('test images loaded: ', len(test))\n",
    "\n",
    "#solution_loaded = np.loadtxt(ROOT_PATH2 + 'solution_matrices')\n",
    "#num_solution = int(solution_loaded.size / (width * height))\n",
    "#solution = solution_loaded.reshape(num_solution, (width*height) // width, width)\n",
    "\n",
    "#print('solution images loaded: ', len(solution))\n",
    "\n",
    "test_img_id = np.loadtxt(ROOT_PATH2 + 'test_img_ids')\n",
    "#solution_img_id = np.loadtxt(ROOT_PATH2 + 'solution_img_ids')\n",
    "\n",
    "print('test id loaded ', len(test_img_id))\n",
    "#print('test, solution ids loaded: ', len(test_img_id), ',', len(solution_img_id))\n",
    "\n",
    "# forming labels of training data\n",
    "y_train = []\n",
    "for index, row in train_label.iterrows():\n",
    "    temp2 = [row['No Finding'], row['Enlarged Cardiomediastinum'], row['Cardiomegaly'],\n",
    "            row['Lung Opacity'], row['Lung Lesion'], row['Edema'], row['Consolidation'],\n",
    "            row['Pneumonia'], row['Atelectasis'], row['Pneumothorax'], row['Pleural Effusion'], \n",
    "            row['Pleural Other'], row['Fracture'], row['Support Devices']]\n",
    "    i = 0 \n",
    "    for val in temp2: \n",
    "        if val != val: # Handles NaN's\n",
    "            temp2[i] = 0.0\n",
    "        i += 1\n",
    "    y_train.append(temp2)\n",
    "\n",
    "print('y_train loaded: ', len(y_train), '\\n\\n')\n",
    "X_train = train\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "\n",
    "xx_train = X_train[:-100000]\n",
    "yy_train = y_train[:-100000]\n",
    "xx_val = X_train[-100000:]\n",
    "yy_val = y_train[-100000:]\n",
    "\n",
    "\n",
    "tuner = set_up_tuner('val_accuracy', 3, 2,  True, 'CS156b', 'tuner_creation')\n",
    "# implementing the hyperparameters \n",
    "# tuning class \n",
    "run_tuner(tuner, X_train, y_train, 10, xx_train, yy_train, xx_val, yy_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
